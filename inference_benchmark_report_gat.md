# B√°o C√°o Inference Benchmark - GAT Models

**Ng√†y th·ª±c hi·ªán:** 27/11/2025  
**Inference method:** Beam Search  
**Device:** CPU

---

## 1. T·ªïng Quan

B√°o c√°o n√†y tr√¨nh b√†y k·∫øt qu·∫£ inference benchmark c·ªßa c√°c m√¥ h√¨nh Graph Attention Network (GAT) so v·ªõi Baseline v√† Transformer tr√™n c√°c benchmark instances th·ª±c t·∫ø c·ªßa b√†i to√°n OPTW.

### C√°c M√¥ H√¨nh ƒê∆∞·ª£c ƒê√°nh Gi√°
- **Baseline**: M√¥ h√¨nh baseline ban ƒë·∫ßu v·ªõi LSTM encoder-decoder
- **Transformer**: M√¥ h√¨nh v·ªõi Transformer decoder
- **GAT-LSTM**: M√¥ h√¨nh GAT Encoder + LSTM Decoder (ch·ªâ c101)
- **GAT-Transformer**: M√¥ h√¨nh GAT Encoder + Transformer Decoder

### Ph∆∞∆°ng Ph√°p Inference
- **Beam Search** (bs): T√¨m ki·∫øm v·ªõi beam size = 128
- **Test tr√™n benchmark instances th·ª±c t·∫ø** (kh√¥ng ph·∫£i generated data)
- **Metrics ƒëo l∆∞·ªùng:**
  - **Score**: T·ªïng ƒëi·ªÉm thu ƒë∆∞·ª£c (c√†ng cao c√†ng t·ªët)
  - **Inference Time**: Th·ªùi gian inference t√≠nh b·∫±ng milliseconds (c√†ng th·∫•p c√†ng t·ªët)

---

## 2. K·∫øt Qu·∫£ Chi Ti·∫øt

### 2.1 Instance: c101

| Model | Epoch | Score | Time (ms) | Score vs Baseline | Speed vs Baseline |
|-------|-------|-------|-----------|-------------------|-------------------|
| **Baseline** | 50000 | **320** | **1,474** | - | - |
| **Transformer** | 22000 | **320** | 2,064 | = | 1.4x slower |
| **GAT-LSTM** | 200 | **320** | 5,051 | = | 3.4x slower |
| **GAT-Transformer** | 5000 | **320** | 15,261 | = | 10.4x slower |

**Nh·∫≠n x√©t:**
- ‚úÖ T·∫•t c·∫£ 4 m√¥ h√¨nh ƒë·ªÅu ƒë·∫°t **score t·ªëi ∆∞u 320**
- ‚ö†Ô∏è Baseline **nhanh nh·∫•t** (1.47s)
- ‚ö†Ô∏è GAT-Transformer **ch·∫≠m nh·∫•t** (15.26s) - ch·∫≠m h∆°n Baseline 10.4 l·∫ßn
- üìä GAT-LSTM v·ªõi epoch 200 v·∫´n ƒë·∫°t score t·ªëi ∆∞u nh∆∞ng ch·∫≠m h∆°n 3.4 l·∫ßn

### 2.2 Instance: r101

| Model | Epoch | Score | Time (ms) | Score vs Baseline | Speed vs Baseline |
|-------|-------|-------|-----------|-------------------|-------------------|
| **Baseline** | 4000 | **198** | **1,699** | - | - |
| **Transformer** | 4000 | **198** | 2,045 | = | 1.2x slower |
| **GAT-LSTM** | - | N/A | N/A | - | - |
| **GAT-Transformer** | 5000 | **198** | 9,717 | = | 5.7x slower |

**Nh·∫≠n x√©t:**
- ‚úÖ C·∫£ 3 m√¥ h√¨nh ƒë·ªÅu ƒë·∫°t **c√πng score 198**
- ‚ö†Ô∏è GAT-Transformer ch·∫≠m h∆°n Baseline **5.7 l·∫ßn**
- ‚ÑπÔ∏è GAT-LSTM ch∆∞a ƒë∆∞·ª£c train cho instance n√†y

### 2.3 Instance: rc101

| Model | Epoch | Score | Time (ms) | Score vs Baseline | Speed vs Baseline |
|-------|-------|-------|-----------|-------------------|-------------------|
| **Baseline** | 4000 | 219 | **2,240** | - | - |
| **Transformer** | 4000 | 216 | 3,326 | -1.4% | 1.5x slower |
| **GAT-LSTM** | - | N/A | N/A | - | - |
| **GAT-Transformer** | 5000 | **236** ‚≠ê | 8,642 | **+7.8%** | 3.9x slower |

**Nh·∫≠n x√©t:**
- üéØ **GAT-Transformer ƒë·∫°t score cao nh·∫•t: 236** (+7.8% so v·ªõi Baseline)
- ‚ú® C·∫£i thi·ªán ƒë√°ng k·ªÉ so v·ªõi c·∫£ Baseline (219) v√† Transformer (216)
- ‚ö†Ô∏è Trade-off: Ch·∫≠m h∆°n 3.9 l·∫ßn so v·ªõi Baseline
- üìà K·∫øt qu·∫£ ph√π h·ª£p v·ªõi training benchmark (GAT-Trans t·ªët nh·∫•t t·∫°i rc101)

### 2.4 Instance: pr01

| Model | Epoch | Score | Time (ms) | Score vs Baseline | Speed vs Baseline |
|-------|-------|-------|-----------|-------------------|-------------------|
| **Baseline** | 4000 | **299** | **1,471** | - | - |
| **Transformer** | 4000 | 278 | 2,184 | -7.0% | 1.5x slower |
| **GAT-LSTM** | - | N/A | N/A | - | - |
| **GAT-Transformer** | 5000 | **306** ‚≠ê | 16,757 | **+2.3%** | 11.4x slower |

**Nh·∫≠n x√©t:**
- üéØ **GAT-Transformer ƒë·∫°t score cao nh·∫•t: 306** (+2.3% so v·ªõi Baseline)
- ‚úÖ V∆∞·ª£t qua c·∫£ Baseline (299) v√† Transformer (278)
- ‚ö†Ô∏è Inference time r·∫•t ch·∫≠m: 16.76s (ch·∫≠m h∆°n Baseline 11.4 l·∫ßn)
- üîÑ K·∫øt qu·∫£ kh√°c v·ªõi training (training: Baseline t·ªët h∆°n)

### 2.5 Instance: t101

| Model | Epoch | Score | Time (ms) | Score vs Baseline | Speed vs Baseline |
|-------|-------|-------|-----------|-------------------|-------------------|
| **Baseline** | 4000 | 318 | **5,747** | - | - |
| **Transformer** | 4000 | **332** | 7,714 | **+4.4%** | 1.3x slower |
| **GAT-LSTM** | - | N/A | N/A | - | - |
| **GAT-Transformer** | - | N/A | N/A | - | - |

**Nh·∫≠n x√©t:**
- üèÜ **Transformer th·∫Øng** v·ªõi score 332
- ‚ÑπÔ∏è GAT-Transformer ch∆∞a c√≥ model weights cho instance n√†y
- üìä Instance kh√≥, inference time t∆∞∆°ng ƒë·ªëi cao cho c·∫£ 2 models

---

## 3. Ph√¢n T√≠ch T·ªïng H·ª£p

### 3.1 So S√°nh Score (Quality)

#### B·∫£ng T·ªïng H·ª£p Score

| Instance | Winner | Score | Runner-up | ƒêi·ªÉm M·∫°nh |
|----------|--------|-------|-----------|-----------|
| c101 | **Tie (All)** | 320 | - | T·∫•t c·∫£ ƒë·ªÅu t·ªëi ∆∞u |
| r101 | **Tie (3 models)** | 198 | - | C√πng k·∫øt qu·∫£ |
| rc101 | **GAT-Trans** ‚≠ê | 236 | Baseline (219) | +7.8% |
| pr01 | **GAT-Trans** ‚≠ê | 306 | Baseline (299) | +2.3% |
| t101 | **Transformer** | 332 | Baseline (318) | +4.4% |

#### Ph√¢n T√≠ch Wins/Losses

**GAT-Transformer:**
- **Wins**: 2 instances (rc101, pr01) - ƒë·∫°t score cao nh·∫•t
- **Ties**: 2 instances (c101, r101) - c√πng score t·ªëi ∆∞u
- **Losses**: 0 instances (kh√¥ng c√≥ t101 ƒë·ªÉ so s√°nh)
- **T·ªïng th·ªÉ**: Xu·∫•t s·∫Øc v·ªÅ quality

**Transformer:**
- **Wins**: 1 instance (t101)
- **Ties**: 2 instances (c101, r101)
- **Losses**: 2 instances (rc101, pr01)
- **T·ªïng th·ªÉ**: C√¢n b·∫±ng

**Baseline:**
- **Wins**: 0 instances
- **Ties**: 2 instances (c101, r101)
- **Losses**: 2 instances (rc101, pr01)
- **T·ªïng th·ªÉ**: B·ªã v∆∞·ª£t qua b·ªüi GAT-Trans v√† Transformer

### 3.2 So S√°nh Speed (Efficiency)

#### B·∫£ng T·ªïng H·ª£p Inference Time

| Model | Avg Time (ms) | Relative Speed | ƒê√°nh Gi√° |
|-------|---------------|----------------|----------|
| **Baseline** | 2,526 | 1.0x (baseline) | ‚ö° Nhanh nh·∫•t |
| **Transformer** | 3,467 | 1.4x | ‚úÖ Nhanh |
| **GAT-LSTM** | 5,051 | 2.0x | ‚ö†Ô∏è Trung b√¨nh |
| **GAT-Transformer** | 12,594 | **5.0x** | ‚ùå Ch·∫≠m |

> *Avg Time t√≠nh tr√™n c√°c instances c√≥ k·∫øt qu·∫£*

#### Ph√¢n T√≠ch Chi Ti·∫øt Speed

**Worst Case Scenario:**
- GAT-Transformer t·∫°i pr01: **16,757ms** (16.76 gi√¢y)
- **Ch·∫≠m h∆°n Baseline 11.4 l·∫ßn**

**Best Case:**
- Baseline t·∫°i pr01: **1,471ms** (1.47 gi√¢y)

**Speed Ranking:**
1. ü•á Baseline - Fastest
2. ü•à Transformer - 1.4x slower
3. ü•â GAT-LSTM - 2.0x slower  
4. ‚ö†Ô∏è GAT-Transformer - 5.0x slower

### 3.3 Quality vs Speed Trade-off

```
Quality ‚Üí
    ‚îÇ
    ‚îÇ                          GAT-Trans (rc101, pr01)
    ‚îÇ                              ‚≠ê
    ‚îÇ
    ‚îÇ        Transformer (t101)
    ‚îÇ             ‚óè
    ‚îÇ
    ‚îÇ  Baseline (fast but lower quality)
    ‚îÇ      ‚óè
    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Speed
        Faster                             Slower
```

**K·∫øt lu·∫≠n Trade-off:**
- **GAT-Transformer**: High Quality, Low Speed
- **Transformer**: Balanced Quality & Speed
- **Baseline**: High Speed, Lower Quality

---

## 4. Ph√¢n T√≠ch S√¢u

### 4.1 T·∫°i Sao GAT-Transformer Ch·∫≠m?

GAT-Transformer c√≥ inference time ch·∫≠m h∆°n ƒë√°ng k·ªÉ v√¨:

1. **GAT Encoder ph·ª©c t·∫°p h∆°n:**
   - Graph attention mechanism t√≠nh to√°n attention cho t·∫•t c·∫£ c√°c c·∫∑p nodes
   - 3 GAT layers c·∫ßn nhi·ªÅu operations h∆°n
   
2. **Transformer Decoder:**
   - Self-attention mechanism trong decoder
   - Computational cost cao h∆°n LSTM

3. **Beam Search on Complex Model:**
   - Beam search ph·∫£i duy tr√¨ 128 beams
   - M·ªói beam ch·∫°y qua GAT encoder + Transformer decoder
   - Exponential cost increase

**∆Ø·ªõc t√≠nh:**
- GAT Encoder: ~3-4x slower than simple encoder
- Transformer Decoder: ~1.5x slower than LSTM  
- Combined: ~5-6x slower (ph√π h·ª£p v·ªõi k·∫øt qu·∫£ th·ª±c t·∫ø 5.0x)

### 4.2 Score Quality Analysis

**Why GAT-Transformer wins at rc101 and pr01?**

1. **rc101** (Random-Clustered):
   - GAT t·ªët h∆°n trong vi·ªác h·ªçc c·∫•u tr√∫c ƒë·ªì th·ªã ph·ª©c t·∫°p
   - Random-clustered pattern ph√π h·ª£p v·ªõi graph attention
   - Score: 236 vs 219 (Baseline) = **+7.8%**

2. **pr01** (Problem dataset):
   - Dataset ph·ª©c t·∫°p h∆°n
   - GAT + Transformer c√≥ kh·∫£ nƒÉng generalization t·ªët h∆°n
   - Score: 306 vs 299 (Baseline) = **+2.3%**

**Why Baseline/Transformer still competitive?**

1. **c101** (Clustered):
   - Pattern ƒë∆°n gi·∫£n, kh√¥ng c·∫ßn attention ph·ª©c t·∫°p
   - All models converge to optimal (320)

2. **r101** (Random):
   - Optimal solution d·ªÖ t√¨m
   - All models achieve same score (198)

---

## 5. Khuy·∫øn Ngh·ªã S·ª≠ D·ª•ng

### 5.1 Khi N√†o D√πng GAT-Transformer?

‚úÖ **N√™n d√πng khi:**
- C·∫ßn **ch·∫•t l∆∞·ª£ng solution t·ªët nh·∫•t** (v√≠ d·ª•: production planning)
- Instance c√≥ c·∫•u tr√∫c ph·ª©c t·∫°p (random-clustered, mixed patterns)
- Inference time kh√¥ng ph·∫£i v·∫•n ƒë·ªÅ quan tr·ªçng
- C√≥ resource t√≠nh to√°n ƒë·ªß m·∫°nh

‚ùå **Kh√¥ng n√™n d√πng khi:**
- C·∫ßn **real-time inference** (latency-critical applications)
- X·ª≠ l√Ω l∆∞·ª£ng l·ªõn instances (batch processing)
- Resource h·∫°n ch·∫ø (mobile, edge devices)

### 5.2 Khi N√†o D√πng Transformer?

‚úÖ **N√™n d√πng khi:**
- C·∫ßn **c√¢n b·∫±ng gi·ªØa quality v√† speed**
- Application y√™u c·∫ßu response time ~2-3 gi√¢y
- Instance c√≥ ƒë·ªô ph·ª©c t·∫°p trung b√¨nh ƒë·∫øn cao

### 5.3 Khi N√†o D√πng Baseline?

‚úÖ **N√™n d√πng khi:**
- ∆Øu ti√™n **t·ªëc ƒë·ªô inference**
- Instance ƒë∆°n gi·∫£n (clustered patterns)
- Batch processing v·ªõi volume l·ªõn
- Resource-constrained environments

---

## 6. K·∫øt Lu·∫≠n

### 6.1 Key Findings

1. **GAT-Transformer v∆∞·ª£t tr·ªôi v·ªÅ Quality:**
   - ƒê·∫°t score cao nh·∫•t t·∫°i 2/4 instances testable (rc101, pr01)
   - C·∫£i thi·ªán 2.3-7.8% so v·ªõi Baseline
   - Kh√¥ng thua b·∫•t k·ª≥ instance n√†o c√≥ k·∫øt qu·∫£

2. **Trade-off Quality vs Speed r√µ r√†ng:**
   - GAT-Transformer: Best quality, **5x slower**
   - Transformer: Good balance, **1.4x slower**
   - Baseline: Fast, lower quality

3. **Instance-specific Performance:**
   - Clustered (c101): All models t·ªët nh∆∞ nhau
   - Random (r101): All models t·ªët nh∆∞ nhau  
   - Random-Clustered (rc101): GAT-Transformer t·ªët nh·∫•t (+7.8%)
   - Problem dataset (pr01): GAT-Transformer t·ªët nh·∫•t (+2.3%)

### 6.2 Recommendations

#### Cho Research/Development:
1. ‚úÖ **Optimize GAT-Transformer inference speed:**
   - Reduce beam size cho real-time applications
   - Implement model quantization
   - Try greedy decoding as alternative

2. ‚úÖ **Train GAT-LSTM cho more instances:**
   - C√≥ potential cho better speed/quality trade-off
   - Ch·ªâ c√≥ c101 results, c·∫ßn th√™m data

3. ‚úÖ **Complete GAT-Transformer training cho t101:**
   - Missing data point quan tr·ªçng

#### Cho Production:
1. üéØ **Use case specific selection:**
   - High-value planning ‚Üí GAT-Transformer
   - Real-time routing ‚Üí Baseline or Transformer
   - Balanced scenarios ‚Üí Transformer

2. üìä **Consider ensemble approach:**
   - Fast first-pass v·ªõi Baseline
   - Refinement v·ªõi GAT-Transformer cho selected instances

---

## 7. D·ªØ Li·ªáu Th√¥

### Complete Results Table

```
Instance   | Model            | Epoch    | Score    | Time(ms)  
-----------|------------------|----------|----------|----------
c101       | Baseline         | 50000    | 320      | 1474      
c101       | Transformer      | 22000    | 320      | 2064      
c101       | GAT-LSTM         | 200      | 320      | 5051      
c101       | GAT-Transformer  | 5000     | 320      | 15261     
r101       | Baseline         | 4000     | 198      | 1699      
r101       | Transformer      | 4000     | 198      | 2045      
r101       | GAT-LSTM         | N/A      | N/A      | N/A       
r101       | GAT-Transformer  | 5000     | 198      | 9717      
rc101      | Baseline         | 4000     | 219      | 2240      
rc101      | Transformer      | 4000     | 216      | 3326      
rc101      | GAT-LSTM         | N/A      | N/A      | N/A       
rc101      | GAT-Transformer  | 5000     | 236      | 8642      
pr01       | Baseline         | 4000     | 299      | 1471      
pr01       | Transformer      | 4000     | 278      | 2184      
pr01       | GAT-LSTM         | N/A      | N/A      | N/A       
pr01       | GAT-Transformer  | 5000     | 306      | 16757     
t101       | Baseline         | 4000     | 318      | 5747      
t101       | Transformer      | 4000     | 332      | 7714      
t101       | GAT-LSTM         | N/A      | N/A      | N/A       
t101       | GAT-Transformer  | N/A      | N/A      | N/A       
```

### CSV Export
K·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß ƒë∆∞·ª£c l∆∞u t·∫°i: [inference_benchmark_results.csv](file:///home/huyngo/Project/ML/optw_rl/inference_benchmark_results.csv)

---

**T√≥m l·∫°i:** GAT-Transformer l√† m√¥ h√¨nh ch·∫•t l∆∞·ª£ng cao nh·∫•t nh∆∞ng c√≥ trade-off v·ªÅ t·ªëc ƒë·ªô. L·ª±a ch·ªçn model ph·ª• thu·ªôc v√†o requirements c·ª• th·ªÉ c·ªßa application.

**C·∫≠p nh·∫≠t:** 27/11/2025 20:04 GMT+7
