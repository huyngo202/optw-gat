# B√°o C√°o Benchmark GAT Models - OPTW Problem

**Ng√†y t·∫°o:** 27/11/2025  
**Ng∆∞·ªùi th·ª±c hi·ªán:** Benchmark GAT Runner

---

## 1. T·ªïng Quan

B√°o c√°o n√†y t·ªïng h·ª£p k·∫øt qu·∫£ benchmark c√°c m√¥ h√¨nh Graph Attention Network (GAT) cho b√†i to√°n Orienteering Problem with Time Windows (OPTW). C√°c m√¥ h√¨nh ƒë∆∞·ª£c ƒë√°nh gi√° bao g·ªìm:

- **Baseline**: M√¥ h√¨nh baseline ban ƒë·∫ßu
- **Transformer**: M√¥ h√¨nh v·ªõi Transformer Decoder
- **Transformer-PPO**: M√¥ h√¨nh Transformer hu·∫•n luy·ªán v·ªõi Proximal Policy Optimization
- **GAT-Transformer**: M√¥ h√¨nh GAT Encoder + Transformer Decoder
- **GAT-LSTM**: M√¥ h√¨nh GAT Encoder + LSTM Decoder

## 2. Ph∆∞∆°ng Ph√°p Benchmark

### 2.1 T·∫≠p Instance
Benchmark ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n 5 instance ti√™u chu·∫©n:
- **c101**: Clustered customers
- **r101**: Random customers
- **rc101**: Random-clustered customers
- **pr01**: Problem t·ª´ b·ªô d·ªØ li·ªáu chu·∫©n
- **t101**: Time window constrained

### 2.2 C·∫•u H√¨nh Hu·∫•n Luy·ªán
- **S·ªë epochs**: 5,000 (Baseline, Transformer, GAT models)
- **S·ªë epochs**: 10,000 (Transformer-PPO)
- **Batch size**: 16
- **Device**: CPU
- **GAT layers**: 3 layers

### 2.3 Metrics ƒê√°nh Gi√°
- **avg_val**: Average reward tr√™n validation set
- **max_real**: Maximum reward ƒë·∫°t ƒë∆∞·ª£c tr√™n d·ªØ li·ªáu th·ª±c
- **epoch**: S·ªë epoch hu·∫•n luy·ªán

---

## 3. K·∫øt Qu·∫£ Chi Ti·∫øt

### 3.1 Instance: c101

| Model | Epochs | Avg Validation Reward | Max Real Reward |
|-------|--------|----------------------|-----------------|
| **Baseline** | 5000 | 257.16 | **300.0** |
| **Transformer** | 5000 | 254.61 | **300.0** |
| **Transformer-PPO** | 10000 | 172.25 | 10.0 |
| **GAT-Transformer** | 5000 | 256.84 | **300.0** |
| **GAT-LSTM** | 200 | 242.31 | 270.0 |

**Nh·∫≠n x√©t:**
- Ba m√¥ h√¨nh ƒë·∫°t max reward t·ªëi ∆∞u (300.0): Baseline, Transformer, v√† GAT-Transformer
- GAT-LSTM m·ªõi hu·∫•n luy·ªán 200 epochs, ch∆∞a h·ªôi t·ª• ho√†n to√†n
- Transformer-PPO cho k·∫øt qu·∫£ k√©m (max 10.0), c·∫ßn xem x√©t l·∫°i hyperparameters

### 3.2 Instance: r101

| Model | Epochs | Avg Validation Reward | Max Real Reward |
|-------|--------|----------------------|-----------------|
| **Baseline** | 5000 | 109.41 | 190.0 |
| **Transformer** | 5000 | 110.22 | **179.0** |
| **Transformer-PPO** | 10000 | 98.67 | 0.0 |
| **GAT-Transformer** | 5000 | 109.64 | **179.0** |

**Nh·∫≠n x√©t:**
- GAT-Transformer v√† Transformer cho k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng (179.0)
- Avg validation reward c·ªßa GAT-Transformer (109.64) cao nh·∫•t
- Transformer-PPO th·∫•t b·∫°i ho√†n to√†n (max 0.0)

### 3.3 Instance: rc101

| Model | Epochs | Avg Validation Reward | Max Real Reward |
|-------|--------|----------------------|-----------------|
| **Baseline** | 5000 | 147.13 | 202.0 |
| **Transformer** | 5000 | 144.86 | 205.0 |
| **Transformer-PPO** | 10000 | 108.69 | 0.0 |
| **GAT-Transformer** | 5000 | 148.09 | **216.0** |

**Nh·∫≠n x√©t:**
- ‚ú® **GAT-Transformer v∆∞·ª£t tr·ªôi** v·ªõi max reward 216.0 (cao nh·∫•t)
- GAT-Transformer c≈©ng ƒë·∫°t avg validation reward cao nh·∫•t (148.09)
- C·∫£i thi·ªán ~5-7% so v·ªõi Baseline v√† Transformer

### 3.4 Instance: pr01

| Model | Epochs | Avg Validation Reward | Max Real Reward |
|-------|--------|----------------------|-----------------|
| **Baseline** | 5000 | 184.50 | **306.0** |
| **Transformer** | 5000 | 182.94 | 279.0 |
| **Transformer-PPO** | 10000 | 121.81 | 52.0 |
| **GAT-Transformer** | 5000 | 182.84 | 277.0 |

**Nh·∫≠n x√©t:**
- Baseline ƒë·∫°t max reward cao nh·∫•t (306.0)
- GAT-Transformer v√† Transformer c√≥ k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng (~277-279)
- Transformer-PPO k√©m h∆°n ƒë√°ng k·ªÉ

### 3.5 Instance: t101

| Model | Epochs | Avg Validation Reward | Max Real Reward |
|-------|--------|----------------------|-----------------|
| **Baseline** | 5000 | 763.94 | 214.0 |
| **Transformer** | 5000 | 748.80 | **326.0** |
| **Transformer-PPO** | 10000 | 399.98 | 125.0 |
| **GAT-Transformer** | - | - | **N/A** |

**Nh·∫≠n x√©t:**
- GAT-Transformer ch∆∞a c√≥ k·∫øt qu·∫£ hu·∫•n luy·ªán (training history ch∆∞a ƒë∆∞·ª£c t·∫°o)
- Transformer ƒë·∫°t max reward cao nh·∫•t (326.0)
- ƒê√¢y l√† instance kh√≥ nh·∫•t v·ªõi avg validation reward r·∫•t cao (>700)

---

## 4. So S√°nh T·ªïng H·ª£p

### 4.1 B·∫£ng So S√°nh Max Real Reward

| Instance | Baseline | Transformer | PPO | GAT-Trans | GAT-LSTM | **Winner** |
|----------|----------|-------------|-----|-----------|----------|------------|
| c101 | 300.0 | 300.0 | 10.0 | 300.0 | 270.0 | **Tie (3-way)** |
| r101 | 190.0 | 179.0 | 0.0 | 179.0 | - | **Baseline** |
| rc101 | 202.0 | 205.0 | 0.0 | **216.0** | - | **GAT-Trans** |
| pr01 | **306.0** | 279.0 | 52.0 | 277.0 | - | **Baseline** |
| t101 | 214.0 | **326.0** | 125.0 | N/A | - | **Transformer** |

### 4.2 Hi·ªáu Su·∫•t Theo Model

#### GAT-Transformer
- ‚úÖ **ƒêi·ªÉm m·∫°nh:**
  - ƒê·∫°t k·∫øt qu·∫£ t·ªët nh·∫•t t·∫°i rc101 (216.0)
  - T∆∞∆°ng ƒë∆∞∆°ng v·ªõi Transformer t·∫°i c101 v√† r101
  - Avg validation reward cao v√† ·ªïn ƒë·ªãnh
  
- ‚ö†Ô∏è **ƒêi·ªÉm c·∫ßn c·∫£i thi·ªán:**
  - Ch∆∞a c√≥ k·∫øt qu·∫£ cho t101
  - T·∫°i pr01, k√©m h∆°n Baseline ~9.5%

#### GAT-LSTM
- ‚ö†Ô∏è **Tr·∫°ng th√°i:**
  - Ch·ªâ c√≥ k·∫øt qu·∫£ cho c101
  - M·ªõi hu·∫•n luy·ªán 200 epochs (ch∆∞a ƒë·ªß ƒë·ªÉ ƒë√°nh gi√°)
  - C·∫ßn ti·∫øp t·ª•c hu·∫•n luy·ªán ƒë·ªÉ so s√°nh c√¥ng b·∫±ng

#### Transformer-PPO
- ‚ùå **V·∫•n ƒë·ªÅ nghi√™m tr·ªçng:**
  - K·∫øt qu·∫£ k√©m h∆°n r·∫•t nhi·ªÅu so v·ªõi c√°c m√¥ h√¨nh kh√°c
  - Max reward = 0 t·∫°i r101 v√† rc101
  - C·∫ßn review l·∫°i c·∫•u h√¨nh PPO v√† hyperparameters

---

## 5. Ph√¢n T√≠ch v√† ƒê√°nh Gi√°

### 5.1 Hi·ªáu Qu·∫£ c·ªßa GAT Encoder

GAT (Graph Attention Network) encoder ƒë√£ cho th·∫•y kh·∫£ nƒÉng h·ªçc bi·ªÉu di·ªÖn ƒë·ªì th·ªã t·ªët:

1. **T·ªët nh·∫•t t·∫°i rc101:** GAT-Transformer ƒë·∫°t 216.0 (cao nh·∫•t), cho th·∫•y GAT x·ª≠ l√Ω t·ªët b√†i to√°n c√≥ c·∫•u tr√∫c random-clustered

2. **T∆∞∆°ng ƒë∆∞∆°ng v·ªõi Transformer chu·∫©n:** T·∫°i c101 v√† r101, GAT-Transformer ƒë·∫°t k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng ho·∫∑c t·ªët h∆°n m·ªôt ch√∫t so v·ªõi Transformer truy·ªÅn th·ªëng

3. **Avg validation reward cao:** GAT-Transformer th∆∞·ªùng c√≥ avg validation reward cao, cho th·∫•y t√≠nh ·ªïn ƒë·ªãnh

### 5.2 So S√°nh GAT-Transformer vs Transformer

| Metric | GAT-Transformer | Transformer | K·∫øt lu·∫≠n |
|--------|-----------------|-------------|----------|
| **Wins** | 1 (rc101) | 1 (t101) | Ngang nhau |
| **Ties** | 2 (c101, r101) | 2 (c101, r101) | - |
| **Stability** | Cao (avg_val ·ªïn ƒë·ªãnh) | Cao | GAT h∆°i t·ªët h∆°n |
| **Training Speed** | T∆∞∆°ng ƒë∆∞∆°ng | T∆∞∆°ng ƒë∆∞∆°ng | - |

### 5.3 V·∫•n ƒê·ªÅ v·ªõi PPO

Transformer-PPO cho k·∫øt qu·∫£ r·∫•t k√©m tr√™n h·∫ßu h·∫øt c√°c instance:
- Max reward = 0 t·∫°i r101, rc101
- Max reward = 10 t·∫°i c101 (so v·ªõi 300 c·ªßa c√°c m√¥ h√¨nh kh√°c)

**Nguy√™n nh√¢n c√≥ th·ªÉ:**
- Hyperparameters ch∆∞a ph√π h·ª£p
- Learning rate qu√° cao/th·∫•p
- S·ªë epochs ch∆∞a ƒë·ªß (m·∫∑c d√π ƒë√£ train 10,000 epochs)
- C·∫•u tr√∫c reward shaping c·∫ßn ƒëi·ªÅu ch·ªânh

---

## 6. K·∫øt Lu·∫≠n v√† Khuy·∫øn Ngh·ªã

### 6.1 K·∫øt Lu·∫≠n

1. **GAT-Transformer** l√† m·ªôt ki·∫øn tr√∫c h·ª©a h·∫πn:
   - ƒê·∫°t k·∫øt qu·∫£ t·ªët nh·∫•t t·∫°i rc101
   - T∆∞∆°ng ƒë∆∞∆°ng v·ªõi Transformer t·∫°i c√°c instance kh√°c
   - Avg validation reward cao v√† ·ªïn ƒë·ªãnh

2. **GAT-LSTM** c·∫ßn th√™m th·ªùi gian hu·∫•n luy·ªán ƒë·ªÉ ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß

3. **Transformer-PPO** c·∫ßn ƒë∆∞·ª£c xem x√©t l·∫°i to√†n b·ªô

4. **Baseline v√† Transformer** v·∫´n l√† nh·ªØng m√¥ h√¨nh ƒë√°ng tin c·∫≠y

### 6.2 Khuy·∫øn Ngh·ªã

#### ∆Øu ti√™n cao:
1. ‚úÖ **Ho√†n th√†nh hu·∫•n luy·ªán GAT-Transformer cho t101**
2. ‚úÖ **Hu·∫•n luy·ªán GAT-LSTM ƒë·ªß 5000 epochs cho t·∫•t c·∫£ instances**
3. üîß **Debug v√† fix Transformer-PPO:**
   - Review l·∫°i c·∫•u h√¨nh hyperparameters
   - Ki·ªÉm tra reward shaping
   - Xem x√©t gi·∫£m learning rate

#### ∆Øu ti√™n trung b√¨nh:
4. üìä **Th√™m metrics ƒë√°nh gi√°:**
   - Convergence speed
   - Training time
   - Memory usage
   - Inference time

5. üß™ **Th·ª≠ nghi·ªám th√™m:**
   - S·ªë l∆∞·ª£ng GAT layers (hi·ªán t·∫°i: 3)
   - Attention heads
   - Hidden dimensions

#### Nghi√™n c·ª©u th√™m:
6. üìö **Ph√¢n t√≠ch s√¢u h∆°n:**
   - T·∫°i sao GAT-Transformer t·ªët t·∫°i rc101?
   - T·∫°i sao Baseline t·ªët h∆°n t·∫°i pr01?
   - Training curves comparison chi ti·∫øt

---

## 7. D·ªØ Li·ªáu Th√¥

### Summary Data (Epoch, Avg Validation, Max Real)

```
=== c101 ===
baseline_bench       : epoch= 5000, avg_val=257.15625, max_real=300.0
transformer_bench    : epoch= 5000, avg_val=254.609375, max_real=300.0
transformer_ppo      : epoch=10000, avg_val=172.25, max_real=10.0
gat_transformer_bench: epoch= 5000, avg_val=256.84375, max_real=300.0
gat_lstm            : epoch=  200, avg_val=242.3125, max_real=270.0

=== r101 ===
baseline_bench       : epoch= 5000, avg_val=109.40625, max_real=190.0
transformer_bench    : epoch= 5000, avg_val=110.21875, max_real=179.0
transformer_ppo      : epoch=10000, avg_val=98.671875, max_real=0.0
gat_transformer_bench: epoch= 5000, avg_val=109.640625, max_real=179.0

=== rc101 ===
baseline_bench       : epoch= 5000, avg_val=147.125, max_real=202.0
transformer_bench    : epoch= 5000, avg_val=144.859375, max_real=205.0
transformer_ppo      : epoch=10000, avg_val=8.890625, max_real=0.0
gat_transformer_bench: epoch= 5000, avg_val=148.09375, max_real=216.0

=== pr01 ===
baseline_bench       : epoch= 5000, avg_val=184.5, max_real=306.0
transformer_bench    : epoch= 5000, avg_val=170.671875, max_real=279.0
transformer_ppo      : epoch=10000, avg_val=23.0, max_real=52.0
gat_transformer_bench: epoch= 5000, avg_val=180.890625, max_real=277.0

=== t101 ===
baseline_bench       : epoch= 5000, avg_val=763.9375, max_real=214.0
transformer_bench    : epoch= 5000, avg_val=748.796875, max_real=326.0
transformer_ppo      : epoch=10000, avg_val=236.71875, max_real=125.0
gat_transformer_bench: N/A (no training history)
```

---

## Ph·ª• L·ª•c

### A. C·∫•u Tr√∫c M√¥ H√¨nh

#### GAT-Transformer Architecture
```
Input Graph ‚Üí GAT Encoder (3 layers) ‚Üí Transformer Decoder ‚Üí Action Selection
```

#### GAT-LSTM Architecture
```
Input Graph ‚Üí GAT Encoder (3 layers) ‚Üí LSTM Decoder ‚Üí Action Selection
```

### B. Files v√† Scripts
- Training scripts: `train_optw_gat_transformer.py`, `train_optw_gat_lstm.py`
- Benchmark runner: `benchmark_gat_runner.py`
- Results location: `results/{instance}/outputs/model_{model_name}_uni_samp/`

### C. Tham Kh·∫£o
- Previous benchmarks: `benchmark_report.md`, `benchmark_results.md`
- Training curves: `training_curve_*.png`
- Comprehensive report: Generated by `benchmark_report_all.py`

---

**C·∫≠p nh·∫≠t l·∫ßn cu·ªëi:** 27/11/2025 19:47 GMT+7
